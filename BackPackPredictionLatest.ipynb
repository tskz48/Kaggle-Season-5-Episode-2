{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIe16pjRojbl",
        "outputId": "b3c990b4-96d7-42a9-bc22-89f00d899f87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.15.1-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.15.1-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.8/231.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.9 alembic-1.15.1 colorlog-6.9.0 optuna-4.2.1\n",
            "Collecting optuna_integration\n",
            "  Downloading optuna_integration-4.2.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (from optuna_integration) (4.2.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna->optuna_integration) (1.15.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna->optuna_integration) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna->optuna_integration) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna->optuna_integration) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna->optuna_integration) (2.0.38)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna->optuna_integration) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna->optuna_integration) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna->optuna_integration) (1.3.9)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna->optuna_integration) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna->optuna_integration) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna->optuna_integration) (3.0.2)\n",
            "Downloading optuna_integration-4.2.1-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: optuna_integration\n",
            "Successfully installed optuna_integration-4.2.1\n",
            "Collecting scikeras\n",
            "  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from scikeras) (3.8.0)\n",
            "Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from scikeras) (1.6.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.14.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (24.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras>=3.2.0->scikeras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
            "Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.13.0\n",
            "Requirement already satisfied: cupy-cuda12x in /usr/local/lib/python3.11/dist-packages (13.3.0)\n",
            "Collecting cupy-cuda12x\n",
            "  Downloading cupy_cuda12x-13.4.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x) (1.26.4)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.11/dist-packages (from cupy-cuda12x) (0.8.3)\n",
            "Downloading cupy_cuda12x-13.4.0-cp311-cp311-manylinux2014_x86_64.whl (105.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cupy-cuda12x\n",
            "  Attempting uninstall: cupy-cuda12x\n",
            "    Found existing installation: cupy-cuda12x 13.3.0\n",
            "    Uninstalling cupy-cuda12x-13.3.0:\n",
            "      Successfully uninstalled cupy-cuda12x-13.3.0\n",
            "Successfully installed cupy-cuda12x-13.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna\n",
        "!pip install optuna_integration\n",
        "!pip install scikeras\n",
        "!pip install -U cupy-cuda12x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vij7THuTpJ5L",
        "outputId": "c2eab9c3-758a-46d4-c6d2-23fe3de848ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gU1E03PoU0O",
        "outputId": "f1aabb79-383a-4a10-fc9f-9309c5481403"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                Brand   Material    Size  Compartments Laptop Compartment  \\\n",
            "0            Jansport    Leather  Medium           7.0                Yes   \n",
            "1            Jansport     Canvas   Small          10.0                Yes   \n",
            "2        Under Armour    Leather   Small           2.0                Yes   \n",
            "3                Nike      Nylon   Small           8.0                Yes   \n",
            "4              Adidas     Canvas  Medium           1.0                Yes   \n",
            "...               ...        ...     ...           ...                ...   \n",
            "3994313          Nike     Canvas     NaN           3.0                Yes   \n",
            "3994314          Puma    Leather   Small          10.0                Yes   \n",
            "3994315      Jansport     Canvas   Large          10.0                 No   \n",
            "3994316          Puma     Canvas     NaN           2.0                 No   \n",
            "3994317  Under Armour  Polyester  Medium           2.0                Yes   \n",
            "\n",
            "        Waterproof      Style  Color  Weight Capacity (kg)      Price  \n",
            "0               No       Tote  Black             11.611723  112.15875  \n",
            "1              Yes  Messenger  Green             27.078537   68.88056  \n",
            "2               No  Messenger    Red             16.643760   39.17320  \n",
            "3               No  Messenger  Green             12.937220   80.60793  \n",
            "4              Yes  Messenger  Green             17.749338   86.02312  \n",
            "...            ...        ...    ...                   ...        ...  \n",
            "3994313        Yes  Messenger   Blue             28.098120  104.74460  \n",
            "3994314        Yes       Tote   Blue             17.379531  122.39043  \n",
            "3994315         No   Backpack    Red             17.037708  148.18470  \n",
            "3994316         No   Backpack   Gray             28.783339   22.32269  \n",
            "3994317         No   Backpack   Blue             23.076169  107.61199  \n",
            "\n",
            "[3994318 rows x 10 columns]\n",
            "                Brand   Material    Size  Compartments Laptop Compartment  \\\n",
            "0            Jansport    Leather  Medium           7.0                Yes   \n",
            "1            Jansport     Canvas   Small          10.0                Yes   \n",
            "2        Under Armour    Leather   Small           2.0                Yes   \n",
            "3                Nike      Nylon   Small           8.0                Yes   \n",
            "4              Adidas     Canvas  Medium           1.0                Yes   \n",
            "...               ...        ...     ...           ...                ...   \n",
            "3994313          Nike     Canvas     NaN           3.0                Yes   \n",
            "3994314          Puma    Leather   Small          10.0                Yes   \n",
            "3994315      Jansport     Canvas   Large          10.0                 No   \n",
            "3994316          Puma     Canvas     NaN           2.0                 No   \n",
            "3994317  Under Armour  Polyester  Medium           2.0                Yes   \n",
            "\n",
            "        Waterproof      Style  Color  Weight Capacity (kg)  \n",
            "0               No       Tote  Black             11.611723  \n",
            "1              Yes  Messenger  Green             27.078537  \n",
            "2               No  Messenger    Red             16.643760  \n",
            "3               No  Messenger  Green             12.937220  \n",
            "4              Yes  Messenger  Green             17.749338  \n",
            "...            ...        ...    ...                   ...  \n",
            "3994313        Yes  Messenger   Blue             28.098120  \n",
            "3994314        Yes       Tote   Blue             17.379531  \n",
            "3994315         No   Backpack    Red             17.037708  \n",
            "3994316         No   Backpack   Gray             28.783339  \n",
            "3994317         No   Backpack   Blue             23.076169  \n",
            "\n",
            "[3994318 rows x 9 columns]\n",
            "0          112.15875\n",
            "1           68.88056\n",
            "2           39.17320\n",
            "3           80.60793\n",
            "4           86.02312\n",
            "             ...    \n",
            "3994313    104.74460\n",
            "3994314    122.39043\n",
            "3994315    148.18470\n",
            "3994316     22.32269\n",
            "3994317    107.61199\n",
            "Name: Price, Length: 3994318, dtype: float64\n",
            "RangeIndex(start=0, stop=3994318, step=1)\n",
            "Brand                    object\n",
            "Material                 object\n",
            "Size                     object\n",
            "Compartments            float64\n",
            "Laptop Compartment       object\n",
            "Waterproof               object\n",
            "Style                    object\n",
            "Color                    object\n",
            "Weight Capacity (kg)    float64\n",
            "dtype: object\n",
            "Brand                   126758\n",
            "Material                110962\n",
            "Size                     87785\n",
            "Compartments                 0\n",
            "Laptop Compartment       98533\n",
            "Waterproof               94324\n",
            "Style                   104180\n",
            "Color                   133617\n",
            "Weight Capacity (kg)      1808\n",
            "dtype: int64\n",
            "Brand                   6227\n",
            "Material                5613\n",
            "Size                    4381\n",
            "Compartments               0\n",
            "Laptop Compartment      4962\n",
            "Waterproof              4811\n",
            "Style                   5153\n",
            "Color                   6785\n",
            "Weight Capacity (kg)      77\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-c981278e6a41>:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_train['Weight Capacity (kg)'].fillna(X_train['Weight Capacity (kg)'].median(), inplace=True)\n",
            "<ipython-input-3-c981278e6a41>:50: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_test['Weight Capacity (kg)'].fillna(X_train['Weight Capacity (kg)'].median(), inplace=True)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "import os\n",
        "import optuna\n",
        "from optuna.integration import TFKerasPruningCallback\n",
        "from optuna.pruners import HyperbandPruner\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, Callback\n",
        "\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.model_selection import train_test_split,StratifiedKFold, GridSearchCV, KFold\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, OrdinalEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from cuml.preprocessing import TargetEncoder\n",
        "\n",
        "df_train = pd.read_csv(\"/content/drive/MyDrive/train.csv\",index_col='id') #\n",
        "df_train_ex=pd.read_csv(\"/content/drive/MyDrive/training_extra.csv\",index_col='id')\n",
        "df_test = pd.read_csv(\"/content/drive/MyDrive/test.csv\",index_col='id')\n",
        "df_train=pd.concat([df_train, df_train_ex], axis=0,ignore_index=True) #Data integration/merging of the training dataset with the extended dataset.\n",
        "df_train=df_train.reset_index(drop=True)\n",
        "# df_train.index = df_train['id']\n",
        "# df_train.index.name = None\n",
        "#df_train.rename(columns={df_train.columns[0]: \"\"}, inplace=True)\n",
        "print(df_train)\n",
        "#df_train.reset_index(drop=True, inplace=True)  # Reset the index\n",
        "#df_train.set_index(\"id\", inplace=True)\n",
        "#df_train[\"id\"] = df_train.index\n",
        "X_train = df_train.drop(columns=['Price']) #We only want the features in X_train, so we remove Price (the target variable). We also remove 'id' as it does not influence the price of the backpack. 'Id' is only an identifier for the sample.\n",
        "y_train = df_train['Price']\n",
        "print(X_train)\n",
        "print(y_train)\n",
        "print(X_train.index)\n",
        "X_test= copy.deepcopy(df_test) #This creates a duplicate copy of df_test. Note that since this is a deepcopy, any changes to X_test will not affect df_test, as opposed to a shallow copy where both objects will be changed.\n",
        "#X_test.drop(columns=['id'],inplace=True)\n",
        "\n",
        "print(X_train.dtypes)\n",
        "print(X_train.isnull().sum()) #Checks that number of missing values in each column\n",
        "print(X_test.isnull().sum())\n",
        "\n",
        "#X_train['Compartments'].fillna(X_train['Compartments'].median(), inplace=True) #Replacing missing values in numeric columns with the median of that column, for the training data.\n",
        "X_train['Weight Capacity (kg)'].fillna(X_train['Weight Capacity (kg)'].median(), inplace=True)\n",
        "\n",
        "#X_test['Compartments'].fillna(X_test['Compartments'].median(), inplace=True) #Missing value imputation for testing data, indentical process to training data.\n",
        "X_test['Weight Capacity (kg)'].fillna(X_train['Weight Capacity (kg)'].median(), inplace=True)\n",
        "# def impute_categorical_uniform(df, columns):\n",
        "#     \"\"\"\n",
        "#     Imputes missing values in a categorical column using a uniform distribution.\n",
        "\n",
        "#     Args:\n",
        "#     df (pd.DataFrame): Input dataframe.\n",
        "#     columns: List of column names to impute.\n",
        "\n",
        "#     Returns:\n",
        "#     pd.DataFrame: Dataframe with imputed values.\n",
        "#     \"\"\"\n",
        "#     df_copy = df.copy()\n",
        "#     for column in columns:\n",
        "#         # Find unique categories (excluding NaN)\n",
        "\n",
        "#         # Find unique categories (excluding NaN)\n",
        "#         #unique_categories = df[column].dropna().unique()\n",
        "\n",
        "#     # Count missing values\n",
        "#         num_missing = df[column].isna().sum()\n",
        "\n",
        "#     # If there are missing values, sample from the unique categories\n",
        "#         if num_missing > 0:\n",
        "\n",
        "\n",
        "#                 # Sample based on observed category probabilities\n",
        "#             category_probs = df_copy[column].value_counts(normalize=True)\n",
        "#             imputed_values = np.random.choice(category_probs.index, size=num_missing, p=category_probs.values)\n",
        "#             df_copy.loc[df_copy[column].isna(), column] = imputed_values\n",
        "\n",
        "#     return df_copy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# X_train=impute_categorical_uniform(X_train, columns=['Brand', 'Material', 'Laptop Compartment', 'Waterproof', 'Style', 'Color', 'Size'])\n",
        "# X_test = impute_categorical_uniform (X_test, columns=['Brand', 'Material', 'Laptop Compartment', 'Waterproof', 'Style', 'Color', 'Size'])\n",
        "\n",
        "obj_cols = X_test.select_dtypes(include=['object']).columns\n",
        "X_train[obj_cols] = X_train[obj_cols].fillna('None')\n",
        "X_test[obj_cols] = X_test[obj_cols].fillna('None')\n",
        "\n",
        "# def target_encode_blended(X_train, y_train, X_test, cat_columns, alpha=10, n_splits=25):  #target encoding with smoothing parameter alpha and cross validation. This converts the categorical columns to numeric, using the corresponding mean values of the dependent variable\n",
        "#     \"\"\"\n",
        "#     Performs target encoding with blending without concatenating X_train and y_train.\n",
        "\n",
        "#     Args:\n",
        "#         X_train (pd.DataFrame): Training feature dataframe (without the target column).\n",
        "#         y_train (pd.Series): Target variable.\n",
        "#         cat_columns (list): List of categorical columns to encode.\n",
        "#         alpha (int): Smoothing factor (higher values make it closer to the global mean).\n",
        "#         n_splits (int): Number of splits for KFold cross-validation.\n",
        "\n",
        "#     Returns:\n",
        "#         pd.DataFrame: Transformed training dataset with encoded columns.\n",
        "#     \"\"\"\n",
        "#     train_encoded = X_train.copy()\n",
        "#     test_encoded = X_test.copy()\n",
        "#     global_mean = y_train.mean()\n",
        "\n",
        "#     # Initialize new encoded columns\n",
        "#     for col in cat_columns:\n",
        "#         train_encoded[col + '_target_enc'] = np.nan\n",
        "\n",
        "#     # KFold cross-validation for encoding\n",
        "#     kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "#     for train_idx, val_idx in kf.split(X_train):\n",
        "#         fold_X_train, fold_X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "#         fold_y_train = y_train.iloc[train_idx]\n",
        "\n",
        "#         for col in cat_columns:\n",
        "#             # Compute smoothed mean for each category\n",
        "#             category_means = fold_y_train.groupby(fold_X_train[col]).agg(['mean', 'count'])\n",
        "#             category_means['smooth'] = (category_means['count'] * category_means['mean'] + alpha * global_mean) / (category_means['count'] + alpha)\n",
        "\n",
        "#             # Map smoothed values to the validation set\n",
        "#             train_encoded.loc[X_train.index[val_idx], col + '_target_enc'] = fold_X_val[col].map(category_means['smooth'])\n",
        "\n",
        "#     final_mapping = {}\n",
        "#     for col in cat_columns:\n",
        "#         category_means = y_train.groupby(X_train[col]).agg(['mean', 'count'])\n",
        "#         category_means['smooth'] = (category_means['count'] * category_means['mean'] + alpha * global_mean) / (category_means['count'] + alpha)\n",
        "\n",
        "#         final_mapping[col] = category_means['smooth']\n",
        "\n",
        "#         test_encoded[col + '_target_enc'] = X_test[col].map(final_mapping[col])\n",
        "\n",
        "\n",
        "#     return train_encoded,test_encoded\n",
        "\n",
        "TE = TargetEncoder(n_folds=25, smooth=10, split_method='random', stat='mean')\n",
        "\n",
        "features = X_test.columns.tolist()\n",
        "\n",
        "for col in features:\n",
        "    TE.fit(X_train[col],y_train)\n",
        "    X_train[col] = TE.transform(X_train[col])\n",
        "    X_test[col] = TE.transform(X_test[col])\n",
        "\n",
        "\n",
        "\n",
        "scaler = StandardScaler()     #standardisation of features to have a mean of 0 and variance of 1.\n",
        "columns_to_standardize = X_test.columns #['Compartments', 'Weight Capacity (kg)']\n",
        "X_train[columns_to_standardize] = scaler.fit_transform(X_train[columns_to_standardize])\n",
        "X_test[columns_to_standardize] = scaler.transform(X_test[columns_to_standardize])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "ov4eAw5GoU0R",
        "outputId": "98ce4172-6602-4647-88ad-c6340f82176e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-e3a1aa7a57fc>:8: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "Available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "GPU enabled: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
            "Epoch 1/30 - Val RMSE: 38.88510513305664\n",
            "Epoch 2/30 - Val RMSE: 38.674537658691406\n",
            "Epoch 3/30 - Val RMSE: 38.66902160644531\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e3a1aa7a57fc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_train_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mCustomCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#the higher the value of verbose, the more detailed logs are output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfinal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#Predicting on the testing data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedIterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m       results = tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    879\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1681\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1683\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1684\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1685\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "from tensorflow.keras import mixed_precision\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#mixed_precision.set_global_policy('mixed_float16')\n",
        "print(tf.test.is_built_with_cuda())  # Should return True\n",
        "print(tf.test.is_gpu_available())\n",
        "print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))\n",
        "tf.config.optimizer.set_jit(False) #disable XLA compilation as I was getting an error with it.\n",
        "\n",
        "\n",
        "tf.keras.mixed_precision.set_global_policy('float32')\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "if gpus:\n",
        "    try:\n",
        "        # Set TensorFlow to use only the first GPU\n",
        "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "        print(\"GPU enabled:\", gpus[0])\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "# #Define K for K-Fold Cross-Validation\n",
        "# K = 5  # Number of folds\n",
        "# kf = KFold(n_splits=K, shuffle=True, random_state=42)  # Ensure randomness\n",
        "\n",
        "#target_encode_columns = ['Brand', 'Material', 'Laptop Compartment', 'Waterproof', 'Style', 'Size', 'Color']\n",
        "\n",
        "\n",
        "\n",
        "# #Optuna hyperparameter tuning framework for optimisation.\n",
        "# def objective(trial):\n",
        "\n",
        "#     #Create fresh copies to avoid modifying original data\n",
        "#      X_train_local = copy.deepcopy(X_train)\n",
        "#      X_test_local = copy.deepcopy(X_test)\n",
        "#      y_train_local = copy.deepcopy(y_train)\n",
        "\n",
        "#      alpha = trial.suggest_float(\"alpha\", 10, 12,step=1)\n",
        "#      #learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)\n",
        "#      #l1_l2_reg = trial.suggest_loguniform(\"l1_l2_reg\", 1e-5, 1e-2).              #here we define the possible values for the hyperparameters to test. The hyperparameters are alpha (smoothing parameter for target encoding), the learning rate, the number of neurons in each layer, regularisation term and the batch size.\n",
        "#      #num_units_1 = trial.suggest_int(\"num_units_1\", 64, 512, step=32)\n",
        "#      #num_units_2 = trial.suggest_int(\"num_units_2\", 32, 256, step=16)\n",
        "#      #num_units_3= trial.suggest_int(\"num_units_3\", 16, 128, step=16)\n",
        "#      #num_units_4= trial.suggest_int(\"num_units_4\", 8, 64, step=16)\n",
        "#      #batch_size = trial.suggest_categorical(\"batch_size\", [128, 256, 512]).\n",
        "#      print(trial.params)\n",
        "#      X_train_local, X_test_local =target_encode_blended(X_train_local,y_train_local, X_test_local, target_encode_columns, alpha=alpha)\n",
        "#      # Drop original categorical columns (they are replaced with target encodings)\n",
        "#      X_train_local.drop(columns=target_encode_columns, inplace=True)\n",
        "#      X_test_local.drop(columns=target_encode_columns, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "# #Store RMSE for each fold\n",
        "#      val_rmse_scores = []\n",
        "\n",
        "# # Loop through K-folds\n",
        "#      for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_local)):  # Assuming X_train, y_train are defined\n",
        "#       print(f\"\\n Training Fold {fold+1}/{K}...\")\n",
        "\n",
        "#       X_train_sub, X_val = X_train_local.iloc[train_idx], X_train_local.iloc[val_idx]\n",
        "#       y_train_sub, y_val = y_train_local.iloc[train_idx], y_train_local.iloc[val_idx]\n",
        "#       train_dataset = tf.data.Dataset.from_tensor_slices((X_train_sub, y_train_sub))\n",
        "#       train_dataset = train_dataset.batch(2048).prefetch(tf.data.AUTOTUNE)\n",
        "#       val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(2048).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "\n",
        "#       model = keras.Sequential([\n",
        "#          keras.layers.Input(shape=(X_train_sub.shape[1],)),\n",
        "#         #  keras.layers.Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.005)),\n",
        "#         #  keras.layers.Dropout(0.1),\n",
        "#           #keras.layers.Dense(256,activation='relu',kernel_regularizer=regularizers.l2(0.005)),\n",
        "#         #  keras.layers.Dropout(0.1),\n",
        "#           keras.layers.Dense(128,activation='relu',kernel_regularizer=regularizers.l2(0.0001)),\n",
        "#           #keras.layers.BatchNormalization(),\n",
        "#           #keras.layers.Dropout(0.2),\n",
        "#          keras.layers.Dense(128, activation='relu',kernel_regularizer=regularizers.l2(0.0001)),\n",
        "#          #keras.layers.BatchNormalization(),\n",
        "#          #keras.layers.Dropout(0.1),\n",
        "#          keras.layers.Dense(64, activation='relu',kernel_regularizer=regularizers.l2(0.0001)),\n",
        "#          #keras.layers.BatchNormalization(),\n",
        "#          #keras.layers.Dropout(0.2),\n",
        "#          keras.layers.Dense(1, activation='linear')\n",
        "#                                                       ])  #\n",
        "\n",
        "#       optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "#       model.compile(optimizer=optimizer, loss='mse', metrics=[keras.metrics.RootMeanSquaredError()])\n",
        "#       early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) #early stopping ensure that the training process terminates if the validation loss does not decrease after a certain number of epochs. In this case 5.\n",
        "#       lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6) #dynamically adjusts the learning rate, if the validation loss does not decrease after a set number of epochs.\n",
        "# # pruning_callback = TFKerasPruningCallback(trial, \"val_root_mean_squared_error\")\n",
        "\n",
        "#       history = model.fit(train_dataset,\n",
        "#                         epochs=20,\n",
        "#                         verbose=1, validation_data=val_dataset,callbacks=[lr_scheduler])\n",
        "\n",
        "#     # Return validation RMSE as the score to minimize\n",
        "#       val_rmse = history.history['val_root_mean_squared_error'][-1]  # Last epoch RMSE to be the RMSE for the particular fold.\n",
        "#       val_rmse_scores.append(val_rmse)  # Store RMSE score\n",
        "#       print(f\"✅ Fold {fold+1} RMSE: {val_rmse:.4f}\")\n",
        "\n",
        "\n",
        "#       mean_rmse = np.mean(val_rmse_scores) #find average over all folds to be the RMSE for the cross validation and this is the RMSE for the particular combination of hyperparameters.\n",
        "#       std_rmse = np.std(val_rmse_scores)\n",
        "#       print(f\"Cross-validation RMSE: {mean_rmse:.4f} ± {std_rmse:.4f}\")\n",
        "\n",
        "# study = optuna.create_study(direction=\"minimize\")\n",
        "# study.optimize(objective, n_trials=1)\n",
        "\n",
        "\n",
        "# X_train, X_test =target_encode_blended(X_train,y_train, X_test, target_encode_columns, alpha=20)\n",
        "# X_train.drop(columns=target_encode_columns, inplace=True) #using inplace=True ensures that we do not need to create a new dataframe with the alterations. The existing dataframe is updated with the changes.\n",
        "# X_test.drop(columns=target_encode_columns, inplace=True)\n",
        "\n",
        "\n",
        "X_train_id, X_val_id = train_test_split(X_train.index, test_size=0.2, random_state=42)\n",
        "X_train, X_val = X_train.iloc[X_train_id], X_train.iloc[X_val_id]\n",
        "y_train, y_val = y_train.iloc[X_train_id], y_train.iloc[X_val_id]\n",
        "final_model = keras.Sequential([\n",
        "         keras.layers.Input(shape=(X_train.shape[1],)),\n",
        "#         #  keras.layers.Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.005)),\n",
        "#         #  keras.layers.Dropout(0.1),\n",
        "           #keras.layers.Dense(256,activation='relu'),\n",
        "           #keras.layers.BatchNormalization(),\n",
        "           keras.layers.Dropout(0.1),\n",
        "           keras.layers.Dense(128,kernel_regularizer=regularizers.l2(0.0001)),   #128 neurones in the first layer.\n",
        "           keras.layers.BatchNormalization(),\n",
        "           keras.layers.Activation('relu'),\n",
        "            #keras.layers.LeakyReLU(alpha=0.1), #LeakyRelu activation function, which must be defined in a separate layer, as opposed to to standard Relu, which can be defined in the same layer.\n",
        "           #keras.layers.BatchNormalization(),\n",
        "          keras.layers.Dropout(0.1), #dropout layer used to reduce overfitting by randomly deactivating a proportion of neurons, by setting them to 0. Noise is introduced into the training process.\n",
        "          keras.layers.Dense(128,kernel_regularizer=regularizers.l2(0.0001)), #128 neurons in the second layer\n",
        "            #keras.layers.LeakyReLU(alpha=0.1),\n",
        "\n",
        "\n",
        "          keras.layers.BatchNormalization(), #BatchNormalisation stabilises training process.\n",
        "          keras.layers.Activation('relu'),\n",
        "          keras.layers.Dropout(0.1),\n",
        "          keras.layers.Dense(64,kernel_regularizer=regularizers.l2(0.0001)), #64 neurons in the third layer\n",
        "\n",
        "          #keras.layers.LeakyReLU(alpha=0.1),\n",
        "          keras.layers.BatchNormalization(),\n",
        "          keras.layers.Activation('relu'),\n",
        "          keras.layers.Dropout(0.1),\n",
        "          #keras.layers.Dropout(0.1),\n",
        "          keras.layers.Dense(1, activation='linear')\n",
        "                                                       ]) #one output neuron in the final layer, which represents the predicted price of tbe backpack\n",
        "\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam() #Adam (Adaptive Moment estimation) optimiser is used here for stochastic gradient descent.\n",
        "final_model.compile(optimizer=optimizer, loss='mse', metrics=[keras.metrics.RootMeanSquaredError()]) #loss function is mse and evaluation metric is root mean squared error, both are logged after each epoch.\n",
        "class CustomCallback(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print(f'Epoch {epoch+1}/{self.params[\"epochs\"]} - Val RMSE: {logs[\"val_root_mean_squared_error\"]}')\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) #restore_best_weights=True ensures that tensorflow restores the most optimal parameters, that give the lowest validation loss.\n",
        "#lr_scheduler = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, min_lr=1e-6)\n",
        "full_train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(2048).prefetch(tf.data.AUTOTUNE) #mini batch size of 2048 samples. prefetching reduces latency, as one batch is training, the next batch is loading in parallel. This is more efficient instead of the model waiting for the pipeline to load the next batch before continuing with the training process.\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(2048).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "history = final_model.fit(full_train_dataset, epochs=30, verbose=0,callbacks=[early_stopping,CustomCallback()],validation_data=val_dataset) #the higher the value of verbose, the more detailed logs are output.\n",
        "y_pred=final_model.predict(X_test) #Predicting on the testing data.\n",
        "\n",
        "y_pred = pd.DataFrame(y_pred, columns=[\"prediction\"])  # Ensure predictions are converted to a dataframe and that the dataframe has a column name.\n",
        "\n",
        "# Concatenate X_test[\"id\"] with y_pred_df along axis=1 (column-wise)\n",
        "y_pred = pd.concat([df_test['id'], y_pred], axis=1) # We add the id column back to y_pred, which is required for kaggle submission. Since X_test was a deepcopy, the id column is still present in df_test.\n",
        "\n",
        "# Save to CSV\n",
        "y_pred.to_csv(\"y_pred.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Lk7QbiVJ8iU-"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}