{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIe16pjRojbl",
        "outputId": "e314fd1f-39e3-4697-ca7a-5be5ac3f9bc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.9 alembic-1.14.1 colorlog-6.9.0 optuna-4.2.1\n",
            "Collecting optuna_integration\n",
            "  Downloading optuna_integration-4.2.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (from optuna_integration) (4.2.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna->optuna_integration) (1.14.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna->optuna_integration) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna->optuna_integration) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna->optuna_integration) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna->optuna_integration) (2.0.38)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna->optuna_integration) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna->optuna_integration) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna->optuna_integration) (1.3.9)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna->optuna_integration) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna->optuna_integration) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna->optuna_integration) (3.0.2)\n",
            "Downloading optuna_integration-4.2.1-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: optuna_integration\n",
            "Successfully installed optuna_integration-4.2.1\n",
            "Collecting scikeras\n",
            "  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from scikeras) (3.8.0)\n",
            "Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from scikeras) (1.6.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.14.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (24.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras>=3.2.0->scikeras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
            "Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.13.0\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna\n",
        "!pip install optuna_integration\n",
        "!pip install scikeras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vij7THuTpJ5L",
        "outputId": "9889dc4f-7203-4f72-b3cb-3d428b8dd01e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gU1E03PoU0O",
        "outputId": "57c0db91-f71c-45b7-fb10-47101c19c8bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              id         Brand   Material    Size  Compartments  \\\n",
            "0              0      Jansport    Leather  Medium           7.0   \n",
            "1              1      Jansport     Canvas   Small          10.0   \n",
            "2              2  Under Armour    Leather   Small           2.0   \n",
            "3              3          Nike      Nylon   Small           8.0   \n",
            "4              4        Adidas     Canvas  Medium           1.0   \n",
            "...          ...           ...        ...     ...           ...   \n",
            "4194313  4194313          Nike     Canvas     NaN           3.0   \n",
            "4194314  4194314          Puma    Leather   Small          10.0   \n",
            "4194315  4194315      Jansport     Canvas   Large          10.0   \n",
            "4194316  4194316          Puma     Canvas     NaN           2.0   \n",
            "4194317  4194317  Under Armour  Polyester  Medium           2.0   \n",
            "\n",
            "        Laptop Compartment Waterproof      Style  Color  Weight Capacity (kg)  \\\n",
            "0                      Yes         No       Tote  Black             11.611723   \n",
            "1                      Yes        Yes  Messenger  Green             27.078537   \n",
            "2                      Yes         No  Messenger    Red             16.643760   \n",
            "3                      Yes         No  Messenger  Green             12.937220   \n",
            "4                      Yes        Yes  Messenger  Green             17.749338   \n",
            "...                    ...        ...        ...    ...                   ...   \n",
            "4194313                Yes        Yes  Messenger   Blue             28.098120   \n",
            "4194314                Yes        Yes       Tote   Blue             17.379531   \n",
            "4194315                 No         No   Backpack    Red             17.037708   \n",
            "4194316                 No         No   Backpack   Gray             28.783339   \n",
            "4194317                Yes         No   Backpack   Blue             23.076169   \n",
            "\n",
            "             Price  \n",
            "0        112.15875  \n",
            "1         68.88056  \n",
            "2         39.17320  \n",
            "3         80.60793  \n",
            "4         86.02312  \n",
            "...            ...  \n",
            "4194313  104.74460  \n",
            "4194314  122.39043  \n",
            "4194315  148.18470  \n",
            "4194316   22.32269  \n",
            "4194317  107.61199  \n",
            "\n",
            "[3994318 rows x 11 columns]\n",
            "                Brand   Material    Size  Compartments Laptop Compartment  \\\n",
            "0            Jansport    Leather  Medium           7.0                Yes   \n",
            "1            Jansport     Canvas   Small          10.0                Yes   \n",
            "2        Under Armour    Leather   Small           2.0                Yes   \n",
            "3                Nike      Nylon   Small           8.0                Yes   \n",
            "4              Adidas     Canvas  Medium           1.0                Yes   \n",
            "...               ...        ...     ...           ...                ...   \n",
            "4194313          Nike     Canvas     NaN           3.0                Yes   \n",
            "4194314          Puma    Leather   Small          10.0                Yes   \n",
            "4194315      Jansport     Canvas   Large          10.0                 No   \n",
            "4194316          Puma     Canvas     NaN           2.0                 No   \n",
            "4194317  Under Armour  Polyester  Medium           2.0                Yes   \n",
            "\n",
            "        Waterproof      Style  Color  Weight Capacity (kg)  \n",
            "0               No       Tote  Black             11.611723  \n",
            "1              Yes  Messenger  Green             27.078537  \n",
            "2               No  Messenger    Red             16.643760  \n",
            "3               No  Messenger  Green             12.937220  \n",
            "4              Yes  Messenger  Green             17.749338  \n",
            "...            ...        ...    ...                   ...  \n",
            "4194313        Yes  Messenger   Blue             28.098120  \n",
            "4194314        Yes       Tote   Blue             17.379531  \n",
            "4194315         No   Backpack    Red             17.037708  \n",
            "4194316         No   Backpack   Gray             28.783339  \n",
            "4194317         No   Backpack   Blue             23.076169  \n",
            "\n",
            "[3994318 rows x 9 columns]\n",
            "0          112.15875\n",
            "1           68.88056\n",
            "2           39.17320\n",
            "3           80.60793\n",
            "4           86.02312\n",
            "             ...    \n",
            "4194313    104.74460\n",
            "4194314    122.39043\n",
            "4194315    148.18470\n",
            "4194316     22.32269\n",
            "4194317    107.61199\n",
            "Name: Price, Length: 3994318, dtype: float64\n",
            "Brand                    object\n",
            "Material                 object\n",
            "Size                     object\n",
            "Compartments            float64\n",
            "Laptop Compartment       object\n",
            "Waterproof               object\n",
            "Style                    object\n",
            "Color                    object\n",
            "Weight Capacity (kg)    float64\n",
            "dtype: object\n",
            "Brand                   126758\n",
            "Material                110962\n",
            "Size                     87785\n",
            "Compartments                 0\n",
            "Laptop Compartment       98533\n",
            "Waterproof               94324\n",
            "Style                   104180\n",
            "Color                   133617\n",
            "Weight Capacity (kg)      1808\n",
            "dtype: int64\n",
            "Brand                   6227\n",
            "Material                5613\n",
            "Size                    4381\n",
            "Compartments               0\n",
            "Laptop Compartment      4962\n",
            "Waterproof              4811\n",
            "Style                   5153\n",
            "Color                   6785\n",
            "Weight Capacity (kg)      77\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-9e0e37ecd7fd>:44: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_train['Compartments'].fillna(X_train['Compartments'].median(), inplace=True)\n",
            "<ipython-input-3-9e0e37ecd7fd>:45: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_train['Weight Capacity (kg)'].fillna(X_train['Weight Capacity (kg)'].median(), inplace=True)\n",
            "<ipython-input-3-9e0e37ecd7fd>:47: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_test['Compartments'].fillna(X_test['Compartments'].median(), inplace=True)\n",
            "<ipython-input-3-9e0e37ecd7fd>:48: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_test['Weight Capacity (kg)'].fillna(X_train['Weight Capacity (kg)'].median(), inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compartments: 0 outliers detected.\n",
            "Weight Capacity (kg): 0 outliers detected.\n",
            "Price: 0 outliers detected.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "import os\n",
        "import optuna\n",
        "from optuna.integration import TFKerasPruningCallback\n",
        "from optuna.pruners import HyperbandPruner\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.model_selection import train_test_split,StratifiedKFold, GridSearchCV, KFold\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, OrdinalEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "\n",
        "df_train = pd.read_csv(\"/content/drive/MyDrive/train.csv\")\n",
        "df_train_ex=pd.read_csv(\"/content/drive/MyDrive/training_extra.csv\")\n",
        "df_test = pd.read_csv(\"/content/drive/MyDrive/test.csv\")\n",
        "df_train=pd.concat([df_train, df_train_ex], axis=0)\n",
        "#df_train=df_train.reset_index(drop=True)\n",
        "df_train.index = df_train['id']\n",
        "df_train.index.name = None\n",
        "#df_train.rename(columns={df_train.columns[0]: \"\"}, inplace=True)\n",
        "print(df_train)\n",
        "#df_train.reset_index(drop=True, inplace=True)  # Reset the index\n",
        "#df_train.set_index(\"id\", inplace=True)\n",
        "#df_train[\"id\"] = df_train.index\n",
        "X_train = df_train.drop(columns=['id','Price'])\n",
        "y_train = df_train['Price']\n",
        "print(X_train)\n",
        "print(y_train)\n",
        "X_test= copy.deepcopy(df_test)\n",
        "X_test.drop(columns=['id'],inplace=True)\n",
        "\n",
        "print(X_train.dtypes)\n",
        "print(X_train.isnull().sum())\n",
        "print(X_test.isnull().sum())\n",
        "\n",
        "X_train['Compartments'].fillna(X_train['Compartments'].median(), inplace=True)\n",
        "X_train['Weight Capacity (kg)'].fillna(X_train['Weight Capacity (kg)'].median(), inplace=True)\n",
        "\n",
        "X_test['Compartments'].fillna(X_test['Compartments'].median(), inplace=True)\n",
        "X_test['Weight Capacity (kg)'].fillna(X_train['Weight Capacity (kg)'].median(), inplace=True)\n",
        "# def impute_categorical_uniform(df, columns):\n",
        "#     \"\"\"\n",
        "#     Imputes missing values in a categorical column using a uniform distribution.\n",
        "\n",
        "#     Args:\n",
        "#     df (pd.DataFrame): Input dataframe.\n",
        "#     columns: List of column names to impute.\n",
        "\n",
        "#     Returns:\n",
        "#     pd.DataFrame: Dataframe with imputed values.\n",
        "#     \"\"\"\n",
        "#     df_copy = df.copy()\n",
        "#     for column in columns:\n",
        "#         # Find unique categories (excluding NaN)\n",
        "\n",
        "#         # Find unique categories (excluding NaN)\n",
        "#         #unique_categories = df[column].dropna().unique()\n",
        "\n",
        "#     # Count missing values\n",
        "#         num_missing = df[column].isna().sum()\n",
        "\n",
        "#     # If there are missing values, sample from the unique categories\n",
        "#         if num_missing > 0:\n",
        "\n",
        "\n",
        "#                 # Sample based on observed category probabilities\n",
        "#             category_probs = df_copy[column].value_counts(normalize=True)\n",
        "#             imputed_values = np.random.choice(category_probs.index, size=num_missing, p=category_probs.values)\n",
        "#             df_copy.loc[df_copy[column].isna(), column] = imputed_values\n",
        "\n",
        "#     return df_copy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# X_train=impute_categorical_uniform(X_train, columns=['Brand', 'Material', 'Laptop Compartment', 'Waterproof', 'Style', 'Color', 'Size'])\n",
        "# X_test = impute_categorical_uniform (X_test, columns=['Brand', 'Material', 'Laptop Compartment', 'Waterproof', 'Style', 'Color', 'Size'])\n",
        "\n",
        "obj_cols = X_test.select_dtypes(include=['object']).columns\n",
        "X_train[obj_cols] = X_train[obj_cols].fillna('None')\n",
        "X_test[obj_cols] = X_test[obj_cols].fillna('None')\n",
        "\n",
        "\n",
        "def detect_outliers_iqr(df, columns):\n",
        "    \"\"\"\n",
        "    Detects outliers using the IQR method.\n",
        "\n",
        "    Args:\n",
        "    df (pd.DataFrame): Input dataframe.\n",
        "    columns (list): List of numerical columns to check.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: DataFrame with outlier information.\n",
        "    \"\"\"\n",
        "    df_outliers = df.copy()\n",
        "\n",
        "    for col in columns:\n",
        "        Q1 = df_outliers[col].quantile(0.25)\n",
        "        Q3 = df_outliers[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "        # Detect outliers\n",
        "        outliers = df_outliers[(df_outliers[col] < lower_bound) | (df_outliers[col] > upper_bound)]\n",
        "        print(f\"{col}: {len(outliers)} outliers detected.\")\n",
        "\n",
        "    return df_outliers\n",
        "\n",
        "# Detect outliers in numerical columns\n",
        "numerical_cols = ['Compartments', 'Weight Capacity (kg)']\n",
        "\n",
        "detect_outliers_iqr(X_train, numerical_cols)\n",
        "\n",
        "detect_outliers_iqr(pd.DataFrame(y_train, columns=['Price']), ['Price'])\n",
        "# Initialize Isolation Forest\n",
        "# iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
        "\n",
        "# # Fit on training data (excluding target variable)\n",
        "# outliers = iso_forest.fit_predict(X_train)  # Returns 1 (normal) or -1 (outlier)\n",
        "\n",
        "# # Remove detected outliers\n",
        "# X_train_filtered = X_train[outliers == 1]\n",
        "# y_train_filtered = y_train[outliers == 1]\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "columns_to_standardize = ['Compartments', 'Weight Capacity (kg)']\n",
        "X_train[columns_to_standardize] = scaler.fit_transform(X_train[columns_to_standardize])\n",
        "X_test[columns_to_standardize] = scaler.transform(X_test[columns_to_standardize])\n",
        "#X_train.drop('Color',axis=1,inplace=True)\n",
        "\n",
        "\n",
        "def target_encode_blended(X_train, y_train, X_test, cat_columns, alpha=10, n_splits=25):\n",
        "    \"\"\"\n",
        "    Performs target encoding with blending without concatenating X_train and y_train.\n",
        "\n",
        "    Args:\n",
        "        X_train (pd.DataFrame): Training feature dataframe (without the target column).\n",
        "        y_train (pd.Series): Target variable.\n",
        "        cat_columns (list): List of categorical columns to encode.\n",
        "        alpha (int): Smoothing factor (higher values make it closer to the global mean).\n",
        "        n_splits (int): Number of splits for KFold cross-validation.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Transformed training dataset with encoded columns.\n",
        "    \"\"\"\n",
        "    train_encoded = X_train.copy()\n",
        "    test_encoded = X_test.copy()\n",
        "    global_mean = y_train.mean()\n",
        "\n",
        "    # Initialize new encoded columns\n",
        "    for col in cat_columns:\n",
        "        train_encoded[col + '_target_enc'] = np.nan\n",
        "\n",
        "    # KFold cross-validation for encoding\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X_train):\n",
        "        fold_X_train, fold_X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "        fold_y_train = y_train.iloc[train_idx]\n",
        "\n",
        "        for col in cat_columns:\n",
        "            # Compute smoothed mean for each category\n",
        "            category_means = fold_y_train.groupby(fold_X_train[col]).agg(['mean', 'count'])\n",
        "            category_means['smooth'] = (category_means['count'] * category_means['mean'] + alpha * global_mean) / (category_means['count'] + alpha)\n",
        "\n",
        "            # Map smoothed values to the validation set\n",
        "            train_encoded.loc[X_train.index[val_idx], col + '_target_enc'] = fold_X_val[col].map(category_means['smooth'])\n",
        "\n",
        "    final_mapping = {}\n",
        "    for col in cat_columns:\n",
        "        category_means = y_train.groupby(X_train[col]).agg(['mean', 'count'])\n",
        "        category_means['smooth'] = (category_means['count'] * category_means['mean'] + alpha * global_mean) / (category_means['count'] + alpha)\n",
        "\n",
        "        final_mapping[col] = category_means['smooth']\n",
        "\n",
        "        test_encoded[col + '_target_enc'] = X_test[col].map(final_mapping[col])\n",
        "\n",
        "\n",
        "    return train_encoded,test_encoded\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ov4eAw5GoU0R",
        "outputId": "23d93c10-712d-4614-a9e5-c0a84fdd42f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From <ipython-input-4-b4580d756836>:9: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "Available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "GPU enabled: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m1951/1951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5ms/step - loss: 1929.0175 - root_mean_squared_error: 43.3467 - learning_rate: 1.0000e-04\n",
            "Epoch 2/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/early_stopping.py:153: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,root_mean_squared_error\n",
            "  current = self.get_monitor_value(logs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1951/1951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 5ms/step - loss: 1516.9105 - root_mean_squared_error: 38.9472 - learning_rate: 1.0000e-04\n",
            "Epoch 3/30\n",
            "\u001b[1m1951/1951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 1516.9957 - root_mean_squared_error: 38.9483 - learning_rate: 1.0000e-04\n",
            "Epoch 4/30\n",
            "\u001b[1m1951/1951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 1517.0645 - root_mean_squared_error: 38.9492 - learning_rate: 1.0000e-04\n",
            "Epoch 5/30\n",
            "\u001b[1m1951/1951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 1517.0363 - root_mean_squared_error: 38.9489 - learning_rate: 1.0000e-04\n",
            "Epoch 6/30\n",
            "\u001b[1m1951/1951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 1516.7683 - root_mean_squared_error: 38.9454 - learning_rate: 5.0000e-05\n",
            "Epoch 7/30\n",
            "\u001b[1m1951/1951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 1516.7405 - root_mean_squared_error: 38.9451 - learning_rate: 5.0000e-05\n",
            "Epoch 8/30\n",
            "\u001b[1m1951/1951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 1516.7006 - root_mean_squared_error: 38.9445 - learning_rate: 5.0000e-05\n",
            "Epoch 9/30\n",
            "\u001b[1m1951/1951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 1516.6586 - root_mean_squared_error: 38.9440 - learning_rate: 5.0000e-05\n",
            "Epoch 10/30\n",
            "\u001b[1m1951/1951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 1516.6099 - root_mean_squared_error: 38.9434 - learning_rate: 5.0000e-05\n",
            "Epoch 11/30\n",
            "\u001b[1m1951/1951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 1516.5576 - root_mean_squared_error: 38.9427 - learning_rate: 5.0000e-05\n",
            "Epoch 12/30\n",
            "\u001b[1m1951/1951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 1516.5006 - root_mean_squared_error: 38.9420 - learning_rate: 5.0000e-05\n",
            "Epoch 13/30\n",
            "\u001b[1m1951/1951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 1516.4529 - root_mean_squared_error: 38.9414 - learning_rate: 5.0000e-05\n",
            "Epoch 14/30\n",
            "\u001b[1m1951/1951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - loss: 1516.3978 - root_mean_squared_error: 38.9407 - learning_rate: 5.0000e-05\n",
            "Epoch 15/30\n",
            "\u001b[1m1951/1951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 1516.3456 - root_mean_squared_error: 38.9400 - learning_rate: 5.0000e-05\n",
            "Epoch 16/30\n",
            "\u001b[1m1951/1951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 1516.2899 - root_mean_squared_error: 38.9393 - learning_rate: 5.0000e-05\n",
            "Epoch 17/30\n",
            "\u001b[1m1951/1951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - loss: 1516.2418 - root_mean_squared_error: 38.9386 - learning_rate: 5.0000e-05\n",
            "Epoch 18/30\n",
            "\u001b[1m1951/1951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 1516.1942 - root_mean_squared_error: 38.9380 - learning_rate: 5.0000e-05\n",
            "Epoch 19/30\n",
            "\u001b[1m1951/1951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 1516.1637 - root_mean_squared_error: 38.9376 - learning_rate: 5.0000e-05\n",
            "Epoch 20/30\n",
            "\u001b[1m1951/1951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 1516.1010 - root_mean_squared_error: 38.9368 - learning_rate: 5.0000e-05\n",
            "Epoch 21/30\n",
            "\u001b[1m1951/1951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 1516.0625 - root_mean_squared_error: 38.9363 - learning_rate: 5.0000e-05\n",
            "Epoch 22/30\n",
            "\u001b[1m1951/1951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - loss: 1516.0260 - root_mean_squared_error: 38.9359 - learning_rate: 5.0000e-05\n",
            "Epoch 23/30\n",
            "\u001b[1m1951/1951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 6ms/step - loss: 1515.9971 - root_mean_squared_error: 38.9355 - learning_rate: 5.0000e-05\n",
            "Epoch 24/30\n",
            "\u001b[1m1951/1951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 1515.9750 - root_mean_squared_error: 38.9352 - learning_rate: 5.0000e-05\n",
            "Epoch 25/30\n",
            "\u001b[1m1951/1951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 1515.9539 - root_mean_squared_error: 38.9349 - learning_rate: 5.0000e-05\n",
            "Epoch 26/30\n",
            "\u001b[1m1951/1951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 1515.9260 - root_mean_squared_error: 38.9346 - learning_rate: 5.0000e-05\n",
            "Epoch 27/30\n",
            "\u001b[1m1951/1951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - loss: 1515.9192 - root_mean_squared_error: 38.9345 - learning_rate: 5.0000e-05\n",
            "Epoch 28/30\n",
            "\u001b[1m1951/1951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - loss: 1515.8994 - root_mean_squared_error: 38.9342 - learning_rate: 5.0000e-05\n",
            "Epoch 29/30\n",
            "\u001b[1m1951/1951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - loss: 1515.8890 - root_mean_squared_error: 38.9341 - learning_rate: 5.0000e-05\n",
            "Epoch 30/30\n",
            "\u001b[1m1951/1951\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - loss: 1515.8716 - root_mean_squared_error: 38.9339 - learning_rate: 5.0000e-05\n",
            "\u001b[1m6250/6250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step\n"
          ]
        }
      ],
      "source": [
        "# @title Default title text\n",
        "\n",
        "from tensorflow.keras import mixed_precision\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "#mixed_precision.set_global_policy('mixed_float16')\n",
        "print(tf.test.is_built_with_cuda())  # Should return True\n",
        "print(tf.test.is_gpu_available())\n",
        "print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))\n",
        "tf.config.optimizer.set_jit(False)\n",
        "\n",
        "\n",
        "tf.keras.mixed_precision.set_global_policy('float32')\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "if gpus:\n",
        "    try:\n",
        "        # Set TensorFlow to use only the first GPU\n",
        "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "        print(\"GPU enabled:\", gpus[0])\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "#Define K for K-Fold Cross-Validation\n",
        "K = 5  # Number of folds\n",
        "kf = KFold(n_splits=K, shuffle=True, random_state=42)  # Ensure randomness\n",
        "\n",
        "target_encode_columns = ['Brand', 'Material', 'Laptop Compartment', 'Waterproof', 'Style', 'Size', 'Color']\n",
        "\n",
        "# def objective(trial):\n",
        "\n",
        "    # Create fresh copies to avoid modifying original data\n",
        "    #  X_train_local = copy.deepcopy(X_train)\n",
        "    #  X_test_local = copy.deepcopy(X_test)\n",
        "    #  y_train_local = copy.deepcopy(y_train)\n",
        "\n",
        "    #  #alpha = trial.suggest_float(\"alpha\", 10, 12,step=1)\n",
        "    #  #learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-2)\n",
        "    #  #l1_l2_reg = trial.suggest_loguniform(\"l1_l2_reg\", 1e-5, 1e-2)\n",
        "    #  #num_units_1 = trial.suggest_int(\"num_units_1\", 64, 512, step=32)\n",
        "    #  #num_units_2 = trial.suggest_int(\"num_units_2\", 32, 256, step=16)\n",
        "    #  #num_units_3= trial.suggest_int(\"num_units_3\", 16, 128, step=16)\n",
        "    #  #num_units_4= trial.suggest_int(\"num_units_4\", 8, 64, step=16)\n",
        "    #  #batch_size = trial.suggest_categorical(\"batch_size\", [128, 256, 512])\n",
        "    #  print(trial.params)\n",
        "    #  X_train_local, X_test_local =target_encode_blended(X_train_local,y_train_local, X_test_local, target_encode_columns, alpha=20)\n",
        "    #  # Drop original categorical columns (they are replaced with target encodings)\n",
        "    #  X_train_local.drop(columns=target_encode_columns, inplace=True)\n",
        "    #  X_test_local.drop(columns=target_encode_columns, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "# Store RMSE for each fold\n",
        "#      val_rmse_scores = []\n",
        "\n",
        "# # Loop through K-folds\n",
        "#      for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_local)):  # Assuming X_train, y_train are defined\n",
        "#       print(f\"\\n Training Fold {fold+1}/{K}...\")\n",
        "\n",
        "#       X_train_sub, X_val = X_train_local.iloc[train_idx], X_train_local.iloc[val_idx]\n",
        "#       y_train_sub, y_val = y_train_local.iloc[train_idx], y_train_local.iloc[val_idx]\n",
        "#       train_dataset = tf.data.Dataset.from_tensor_slices((X_train_sub, y_train_sub))\n",
        "#       train_dataset = train_dataset.batch(2048).prefetch(tf.data.AUTOTUNE)\n",
        "#       val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(2048).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "\n",
        "#       model = keras.Sequential([\n",
        "#          keras.layers.Input(shape=(X_train_sub.shape[1],)),\n",
        "#         #  keras.layers.Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.005)),\n",
        "#         #  keras.layers.Dropout(0.1),\n",
        "#           #keras.layers.Dense(256,activation='relu',kernel_regularizer=regularizers.l2(0.005)),\n",
        "#         #  keras.layers.Dropout(0.1),\n",
        "#           keras.layers.Dense(128,activation='relu',kernel_regularizer=regularizers.l2(0.0001)),\n",
        "#           #keras.layers.BatchNormalization(),\n",
        "#           #keras.layers.Dropout(0.2),\n",
        "#          keras.layers.Dense(128, activation='relu',kernel_regularizer=regularizers.l2(0.0001)),\n",
        "#          #keras.layers.BatchNormalization(),\n",
        "#          #keras.layers.Dropout(0.1),\n",
        "#          keras.layers.Dense(64, activation='relu',kernel_regularizer=regularizers.l2(0.0001)),\n",
        "#          #keras.layers.BatchNormalization(),\n",
        "#          #keras.layers.Dropout(0.2),\n",
        "#          keras.layers.Dense(1, activation='linear')\n",
        "#                                                       ])  #\n",
        "\n",
        "#       optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "#       model.compile(optimizer=optimizer, loss='mse', metrics=[keras.metrics.RootMeanSquaredError()])\n",
        "#       early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "#       lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
        "# # pruning_callback = TFKerasPruningCallback(trial, \"val_root_mean_squared_error\")\n",
        "\n",
        "#       history = model.fit(train_dataset,\n",
        "#                         epochs=20,\n",
        "#                         verbose=1, validation_data=val_dataset,callbacks=[lr_scheduler])\n",
        "\n",
        "#     # Return validation RMSE as the score to minimize\n",
        "#       val_rmse = history.history['val_root_mean_squared_error'][-1]  # Last epoch RMSE\n",
        "#       val_rmse_scores.append(val_rmse)  # Store RMSE score\n",
        "#       print(f\"✅ Fold {fold+1} RMSE: {val_rmse:.4f}\")\n",
        "\n",
        "\n",
        "#       mean_rmse = np.mean(val_rmse_scores)\n",
        "#       std_rmse = np.std(val_rmse_scores)\n",
        "#       print(f\"Cross-validation RMSE: {mean_rmse:.4f} ± {std_rmse:.4f}\")\n",
        "\n",
        "# study = optuna.create_study(direction=\"minimize\")\n",
        "# study.optimize(objective, n_trials=1)\n",
        "\n",
        "X_train, X_test =target_encode_blended(X_train,y_train, X_test, target_encode_columns, alpha=20)\n",
        "X_train.drop(columns=target_encode_columns, inplace=True)\n",
        "X_test.drop(columns=target_encode_columns, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "final_model = keras.Sequential([\n",
        "#          keras.layers.Input(shape=(X_train.shape[1],)),\n",
        "#         #  keras.layers.Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.005)),\n",
        "#         #  keras.layers.Dropout(0.1),\n",
        "           #keras.layers.Dense(256,activation='relu'),\n",
        "           #keras.layers.BatchNormalization(),\n",
        "#         #  keras.layers.Dropout(0.1),\n",
        "           keras.layers.Dense(128,kernel_regularizer=regularizers.l2(0.0001)),\n",
        "            #eras.layers.BatchNormalization(),\n",
        "            keras.layers.LeakyReLU(alpha=0.1),\n",
        "#         #  keras.layers.BatchNormalization(),\n",
        "           #keras.layers.Dropout(0.1),\n",
        "          keras.layers.Dense(128,kernel_regularizer=regularizers.l2(0.0001)),\n",
        "            keras.layers.LeakyReLU(alpha=0.1),\n",
        "\n",
        "\n",
        "#          #keras.layers.BatchNormalization(),\n",
        "          #keras.layers.Dropout(0.1),\n",
        "          keras.layers.Dense(64,kernel_regularizer=regularizers.l2(0.0001)),\n",
        "          keras.layers.LeakyReLU(alpha=0.1),\n",
        "#          #keras.layers.BatchNormalization(),\n",
        "          #keras.layers.Dropout(0.1),\n",
        "          keras.layers.Dense(1, activation='linear')\n",
        "                                                       ])\n",
        "\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "final_model.compile(optimizer=optimizer, loss='mse', metrics=[keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, min_lr=1e-6)\n",
        "full_train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(2048).prefetch(tf.data.AUTOTUNE)\n",
        "history = final_model.fit(full_train_dataset, epochs=30, verbose=1,callbacks=[early_stopping, lr_scheduler])\n",
        "y_pred=final_model.predict(X_test)\n",
        "\n",
        "y_pred = pd.DataFrame(y_pred, columns=[\"prediction\"])  # Ensure it has a column name\n",
        "\n",
        "# Concatenate X_test[\"id\"] with y_pred_df along axis=1 (column-wise)\n",
        "y_pred = pd.concat([df_test[[\"id\"]], y_pred], axis=1)\n",
        "\n",
        "# Save to CSV\n",
        "y_pred.to_csv(\"y_pred.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Lk7QbiVJ8iU-"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}