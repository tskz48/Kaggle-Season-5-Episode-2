{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIe16pjRojbl",
        "outputId": "fa04c299-1862-4c56-f321-594ae874ccf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.2.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.2.1-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.6/383.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.9 alembic-1.14.1 colorlog-6.9.0 optuna-4.2.1\n",
            "Collecting optuna_integration\n",
            "  Downloading optuna_integration-4.2.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (from optuna_integration) (4.2.1)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna->optuna_integration) (1.14.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna->optuna_integration) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna->optuna_integration) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna->optuna_integration) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna->optuna_integration) (2.0.38)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna->optuna_integration) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna->optuna_integration) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna->optuna_integration) (1.3.9)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna->optuna_integration) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna->optuna_integration) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna->optuna_integration) (3.0.2)\n",
            "Downloading optuna_integration-4.2.1-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: optuna_integration\n",
            "Successfully installed optuna_integration-4.2.1\n",
            "Collecting scikeras\n",
            "  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from scikeras) (3.8.0)\n",
            "Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from scikeras) (1.6.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.14.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (24.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras>=3.2.0->scikeras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
            "Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.13.0\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna\n",
        "!pip install optuna_integration\n",
        "!pip install scikeras\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vij7THuTpJ5L",
        "outputId": "cc22b011-e954-4365-8789-efbb4bc006bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gU1E03PoU0O",
        "outputId": "de628003-8293-41d7-bbaf-55dd7ae3488f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            id         Brand Material    Size  Compartments  \\\n",
            "0            0      Jansport  Leather  Medium           7.0   \n",
            "1            1      Jansport   Canvas   Small          10.0   \n",
            "2            2  Under Armour  Leather   Small           2.0   \n",
            "3            3          Nike    Nylon   Small           8.0   \n",
            "4            4        Adidas   Canvas  Medium           1.0   \n",
            "...        ...           ...      ...     ...           ...   \n",
            "299995  299995        Adidas  Leather   Small           9.0   \n",
            "299996  299996      Jansport  Leather   Large           6.0   \n",
            "299997  299997          Puma   Canvas   Large           9.0   \n",
            "299998  299998        Adidas    Nylon   Small           1.0   \n",
            "299999  299999  Under Armour   Canvas   Small           2.0   \n",
            "\n",
            "       Laptop Compartment Waterproof      Style  Color  Weight Capacity (kg)  \n",
            "0                     Yes         No       Tote  Black             11.611723  \n",
            "1                     Yes        Yes  Messenger  Green             27.078537  \n",
            "2                     Yes         No  Messenger    Red             16.643760  \n",
            "3                     Yes         No  Messenger  Green             12.937220  \n",
            "4                     Yes        Yes  Messenger  Green             17.749338  \n",
            "...                   ...        ...        ...    ...                   ...  \n",
            "299995                 No         No       Tote   Blue             12.730812  \n",
            "299996                 No        Yes       Tote   Blue             26.633182  \n",
            "299997                Yes        Yes   Backpack   Pink             11.898250  \n",
            "299998                 No        Yes       Tote   Pink              6.175738  \n",
            "299999                 No        Yes   Backpack  Black             18.568865  \n",
            "\n",
            "[300000 rows x 10 columns]\n",
            "0         112.15875\n",
            "1          68.88056\n",
            "2          39.17320\n",
            "3          80.60793\n",
            "4          86.02312\n",
            "            ...    \n",
            "299995    129.99749\n",
            "299996     19.85819\n",
            "299997    111.41364\n",
            "299998    115.89080\n",
            "299999     26.72762\n",
            "Name: Price, Length: 300000, dtype: float64\n",
            "id                        int64\n",
            "Brand                    object\n",
            "Material                 object\n",
            "Size                     object\n",
            "Compartments            float64\n",
            "Laptop Compartment       object\n",
            "Waterproof               object\n",
            "Style                    object\n",
            "Color                    object\n",
            "Weight Capacity (kg)    float64\n",
            "Price                   float64\n",
            "dtype: object\n",
            "id                         0\n",
            "Brand                   9705\n",
            "Material                8347\n",
            "Size                    6595\n",
            "Compartments               0\n",
            "Laptop Compartment      7444\n",
            "Waterproof              7050\n",
            "Style                   7970\n",
            "Color                   9950\n",
            "Weight Capacity (kg)     138\n",
            "Price                      0\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-2e83ccdb9b20>:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_train['Compartments'].fillna(X_train['Compartments'].median(), inplace=True)\n",
            "<ipython-input-3-2e83ccdb9b20>:31: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X_train['Weight Capacity (kg)'].fillna(X_train['Weight Capacity (kg)'].median(), inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compartments: 0 outliers detected.\n",
            "Weight Capacity (kg): 0 outliers detected.\n",
            "Price: 0 outliers detected.\n",
            "            id  Size  Compartments  Weight Capacity (kg)  Brand_Jansport  \\\n",
            "0            0     1      0.538408             -0.921466             1.0   \n",
            "1            1     2      1.576198              1.299086             1.0   \n",
            "2            2     2     -1.191240             -0.199023             0.0   \n",
            "3            3     2      0.884338             -0.731166             0.0   \n",
            "4            4     1     -1.537170             -0.040296             0.0   \n",
            "...        ...   ...           ...                   ...             ...   \n",
            "299995  299995     2      1.230268             -0.760800             0.0   \n",
            "299996  299996     0      0.192479              1.235147             1.0   \n",
            "299997  299997     0      1.230268             -0.880330             0.0   \n",
            "299998  299998     2     -1.537170             -1.701904             0.0   \n",
            "299999  299999     2     -1.191240              0.077363             0.0   \n",
            "\n",
            "        Brand_Nike  Brand_Puma  Brand_Under Armour  Material_Leather  \\\n",
            "0              0.0         0.0                 0.0               1.0   \n",
            "1              0.0         0.0                 0.0               0.0   \n",
            "2              0.0         0.0                 1.0               1.0   \n",
            "3              1.0         0.0                 0.0               0.0   \n",
            "4              0.0         0.0                 0.0               0.0   \n",
            "...            ...         ...                 ...               ...   \n",
            "299995         0.0         0.0                 0.0               1.0   \n",
            "299996         0.0         0.0                 0.0               1.0   \n",
            "299997         0.0         1.0                 0.0               0.0   \n",
            "299998         0.0         0.0                 0.0               0.0   \n",
            "299999         0.0         0.0                 1.0               0.0   \n",
            "\n",
            "        Material_Nylon  Material_Polyester  Laptop Compartment_Yes  \\\n",
            "0                  0.0                 0.0                     1.0   \n",
            "1                  0.0                 0.0                     1.0   \n",
            "2                  0.0                 0.0                     1.0   \n",
            "3                  1.0                 0.0                     1.0   \n",
            "4                  0.0                 0.0                     1.0   \n",
            "...                ...                 ...                     ...   \n",
            "299995             0.0                 0.0                     0.0   \n",
            "299996             0.0                 0.0                     0.0   \n",
            "299997             0.0                 0.0                     1.0   \n",
            "299998             1.0                 0.0                     0.0   \n",
            "299999             0.0                 0.0                     0.0   \n",
            "\n",
            "        Waterproof_Yes  Style_Messenger  Style_Tote  Color_Blue  Color_Gray  \\\n",
            "0                  0.0              0.0         1.0         0.0         0.0   \n",
            "1                  1.0              1.0         0.0         0.0         0.0   \n",
            "2                  0.0              1.0         0.0         0.0         0.0   \n",
            "3                  0.0              1.0         0.0         0.0         0.0   \n",
            "4                  1.0              1.0         0.0         0.0         0.0   \n",
            "...                ...              ...         ...         ...         ...   \n",
            "299995             0.0              0.0         1.0         1.0         0.0   \n",
            "299996             1.0              0.0         1.0         1.0         0.0   \n",
            "299997             1.0              0.0         0.0         0.0         0.0   \n",
            "299998             1.0              0.0         1.0         0.0         0.0   \n",
            "299999             1.0              0.0         0.0         0.0         0.0   \n",
            "\n",
            "        Color_Green  Color_Pink  Color_Red  \n",
            "0               0.0         0.0        0.0  \n",
            "1               1.0         0.0        0.0  \n",
            "2               0.0         0.0        1.0  \n",
            "3               1.0         0.0        0.0  \n",
            "4               1.0         0.0        0.0  \n",
            "...             ...         ...        ...  \n",
            "299995          0.0         0.0        0.0  \n",
            "299996          0.0         0.0        0.0  \n",
            "299997          0.0         1.0        0.0  \n",
            "299998          0.0         1.0        0.0  \n",
            "299999          0.0         0.0        0.0  \n",
            "\n",
            "[300000 rows x 20 columns]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import optuna\n",
        "from optuna.integration import TFKerasPruningCallback\n",
        "from optuna.pruners import HyperbandPruner\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.model_selection import train_test_split,StratifiedKFold, GridSearchCV, KFold\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "df_train = pd.read_csv(\"/content/drive/MyDrive/train.csv\")\n",
        "df_test = pd.read_csv(\"/content/drive/MyDrive/train.csv\")\n",
        "X_train = df_train.drop(columns=['Price'])\n",
        "y_train = df_train['Price']\n",
        "print(X_train)\n",
        "print(y_train)\n",
        "X_test= df_test\n",
        "print(df_train.dtypes)\n",
        "print(df_train.isnull().sum())\n",
        "\n",
        "X_train['Compartments'].fillna(X_train['Compartments'].median(), inplace=True)\n",
        "X_train['Weight Capacity (kg)'].fillna(X_train['Weight Capacity (kg)'].median(), inplace=True)\n",
        "def impute_categorical_uniform(df, columns):\n",
        "    \"\"\"\n",
        "    Imputes missing values in a categorical column using a uniform distribution.\n",
        "\n",
        "    Args:\n",
        "    df (pd.DataFrame): Input dataframe.\n",
        "    columns: List of column names to impute.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: Dataframe with imputed values.\n",
        "    \"\"\"\n",
        "    df_copy = df.copy()\n",
        "    for column in columns:\n",
        "        # Find unique categories (excluding NaN)\n",
        "\n",
        "        # Find unique categories (excluding NaN)\n",
        "        #unique_categories = df[column].dropna().unique()\n",
        "\n",
        "    # Count missing values\n",
        "        num_missing = df[column].isna().sum()\n",
        "\n",
        "    # If there are missing values, sample from the unique categories\n",
        "        if num_missing > 0:\n",
        "\n",
        "\n",
        "                # Sample based on observed category probabilities\n",
        "            category_probs = df_copy[column].value_counts(normalize=True)\n",
        "            imputed_values = np.random.choice(category_probs.index, size=num_missing, p=category_probs.values)\n",
        "            df_copy.loc[df_copy[column].isna(), column] = imputed_values\n",
        "\n",
        "    return df_copy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train=impute_categorical_uniform(X_train, columns=['Brand', 'Material', 'Laptop Compartment', 'Waterproof', 'Style', 'Color', 'Size'])\n",
        "\n",
        "\n",
        "\n",
        "def detect_outliers_iqr(df, columns):\n",
        "    \"\"\"\n",
        "    Detects outliers using the IQR method.\n",
        "\n",
        "    Args:\n",
        "    df (pd.DataFrame): Input dataframe.\n",
        "    columns (list): List of numerical columns to check.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: DataFrame with outlier information.\n",
        "    \"\"\"\n",
        "    df_outliers = df.copy()\n",
        "\n",
        "    for col in columns:\n",
        "        Q1 = df_outliers[col].quantile(0.25)\n",
        "        Q3 = df_outliers[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "        # Detect outliers\n",
        "        outliers = df_outliers[(df_outliers[col] < lower_bound) | (df_outliers[col] > upper_bound)]\n",
        "        print(f\"{col}: {len(outliers)} outliers detected.\")\n",
        "\n",
        "    return df_outliers\n",
        "\n",
        "# Detect outliers in numerical columns\n",
        "numerical_cols = ['Compartments', 'Weight Capacity (kg)']\n",
        "\n",
        "detect_outliers_iqr(X_train, numerical_cols)\n",
        "\n",
        "detect_outliers_iqr(pd.DataFrame(y_train, columns=['Price']), ['Price'])\n",
        "# Initialize Isolation Forest\n",
        "# iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
        "\n",
        "# # Fit on training data (excluding target variable)\n",
        "# outliers = iso_forest.fit_predict(X_train)  # Returns 1 (normal) or -1 (outlier)\n",
        "\n",
        "# # Remove detected outliers\n",
        "# X_train_filtered = X_train[outliers == 1]\n",
        "# y_train_filtered = y_train[outliers == 1]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "columns_to_standardize = ['Compartments', 'Weight Capacity (kg)']\n",
        "X_train[columns_to_standardize] = scaler.fit_transform(X_train[columns_to_standardize])\n",
        "#X_train.drop('Color',axis=1,inplace=True)\n",
        "\n",
        "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
        "X_encoded = encoder.fit_transform(X_train[['Brand', 'Material', 'Laptop Compartment', 'Waterproof', 'Style', 'Color']])\n",
        "\n",
        "# Convert to a DataFrame\n",
        "encoded_df = pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out())\n",
        "\n",
        "# Drop original categorical columns from X_train\n",
        "X_train = X_train.drop(columns=['Brand', 'Material', 'Laptop Compartment', 'Waterproof', 'Style', 'Color'])\n",
        "# Merge the transformed data back\n",
        "X_train = pd.concat([X_train, encoded_df], axis=1)\n",
        "encoder2 = LabelEncoder()\n",
        "X_train['Size'] = encoder2.fit_transform(X_train['Size'])\n",
        "print(X_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ov4eAw5GoU0R",
        "outputId": "7dd0fbfa-88ea-471a-8310-de23238d7952"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n",
            "\n",
            " Training Fold 1/5...\n",
            "Epoch 1/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 611584.0625 - root_mean_squared_error: 758.9054 - val_loss: 3657.6812 - val_root_mean_squared_error: 60.4777 - learning_rate: 1.0000e-04\n",
            "Epoch 2/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 2601.8406 - root_mean_squared_error: 50.4431 - val_loss: 3705.5645 - val_root_mean_squared_error: 60.8727 - learning_rate: 1.0000e-04\n",
            "Epoch 3/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2465.6721 - root_mean_squared_error: 49.0332 - val_loss: 3613.8362 - val_root_mean_squared_error: 60.1145 - learning_rate: 1.0000e-04\n",
            "Epoch 4/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2685.2417 - root_mean_squared_error: 50.9623 - val_loss: 3936.2136 - val_root_mean_squared_error: 62.7387 - learning_rate: 1.0000e-04\n",
            "Epoch 5/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2474.3857 - root_mean_squared_error: 49.0963 - val_loss: 3798.6592 - val_root_mean_squared_error: 61.6327 - learning_rate: 1.0000e-04\n",
            "Epoch 6/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 2941.0574 - root_mean_squared_error: 53.3316 - val_loss: 3479.4998 - val_root_mean_squared_error: 58.9867 - learning_rate: 1.0000e-04\n",
            "Epoch 7/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2896.5881 - root_mean_squared_error: 52.9368 - val_loss: 3555.9709 - val_root_mean_squared_error: 59.6314 - learning_rate: 1.0000e-04\n",
            "Epoch 8/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2795.2197 - root_mean_squared_error: 51.9655 - val_loss: 3631.7214 - val_root_mean_squared_error: 60.2632 - learning_rate: 1.0000e-04\n",
            "Epoch 9/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 2863.2825 - root_mean_squared_error: 52.5617 - val_loss: 3413.1416 - val_root_mean_squared_error: 58.4215 - learning_rate: 1.0000e-04\n",
            "Epoch 10/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 2498.6360 - root_mean_squared_error: 49.2797 - val_loss: 3871.1077 - val_root_mean_squared_error: 62.2177 - learning_rate: 1.0000e-04\n",
            "Epoch 11/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2939.1335 - root_mean_squared_error: 53.2212 - val_loss: 3707.8435 - val_root_mean_squared_error: 60.8915 - learning_rate: 1.0000e-04\n",
            "Epoch 12/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2984.7271 - root_mean_squared_error: 53.6972 - val_loss: 3727.6184 - val_root_mean_squared_error: 61.0537 - learning_rate: 1.0000e-04\n",
            "Epoch 13/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 3145.1323 - root_mean_squared_error: 55.0805 - val_loss: 3750.3828 - val_root_mean_squared_error: 61.2398 - learning_rate: 5.0000e-05\n",
            "Epoch 14/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 3133.2395 - root_mean_squared_error: 54.9659 - val_loss: 3722.0046 - val_root_mean_squared_error: 61.0077 - learning_rate: 5.0000e-05\n",
            "Epoch 15/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2922.5969 - root_mean_squared_error: 53.0819 - val_loss: 3702.3181 - val_root_mean_squared_error: 60.8461 - learning_rate: 5.0000e-05\n",
            "Epoch 16/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 3055.2644 - root_mean_squared_error: 54.2686 - val_loss: 3859.1077 - val_root_mean_squared_error: 62.1212 - learning_rate: 2.5000e-05\n",
            "Epoch 17/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 3078.4265 - root_mean_squared_error: 54.4578 - val_loss: 3842.4607 - val_root_mean_squared_error: 61.9870 - learning_rate: 2.5000e-05\n",
            "Epoch 18/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 2694.5830 - root_mean_squared_error: 51.1015 - val_loss: 3645.6177 - val_root_mean_squared_error: 60.3784 - learning_rate: 2.5000e-05\n",
            "Epoch 19/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - loss: 2783.4441 - root_mean_squared_error: 51.7987 - val_loss: 3565.8679 - val_root_mean_squared_error: 59.7143 - learning_rate: 1.2500e-05\n",
            "Epoch 20/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 3064.3079 - root_mean_squared_error: 54.2945 - val_loss: 3633.4016 - val_root_mean_squared_error: 60.2771 - learning_rate: 1.2500e-05\n",
            "Epoch 21/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 3049.7686 - root_mean_squared_error: 54.1614 - val_loss: 3730.5586 - val_root_mean_squared_error: 61.0777 - learning_rate: 1.2500e-05\n",
            "Epoch 22/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 3264.7852 - root_mean_squared_error: 56.0064 - val_loss: 3774.9094 - val_root_mean_squared_error: 61.4397 - learning_rate: 6.2500e-06\n",
            "Epoch 23/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 3218.3145 - root_mean_squared_error: 55.6138 - val_loss: 3646.4409 - val_root_mean_squared_error: 60.3852 - learning_rate: 6.2500e-06\n",
            "Epoch 24/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2900.5659 - root_mean_squared_error: 52.8449 - val_loss: 3556.5527 - val_root_mean_squared_error: 59.6363 - learning_rate: 6.2500e-06\n",
            "Epoch 25/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 3264.7422 - root_mean_squared_error: 55.9970 - val_loss: 3808.7764 - val_root_mean_squared_error: 61.7147 - learning_rate: 3.1250e-06\n",
            "Epoch 26/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - loss: 3075.4856 - root_mean_squared_error: 54.3241 - val_loss: 3812.8733 - val_root_mean_squared_error: 61.7479 - learning_rate: 3.1250e-06\n",
            "Epoch 27/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 3135.5308 - root_mean_squared_error: 54.8504 - val_loss: 3724.3181 - val_root_mean_squared_error: 61.0266 - learning_rate: 3.1250e-06\n",
            "Epoch 28/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 3356.7683 - root_mean_squared_error: 56.7531 - val_loss: 3673.6824 - val_root_mean_squared_error: 60.6104 - learning_rate: 1.5625e-06\n",
            "Epoch 29/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 3217.0181 - root_mean_squared_error: 55.5430 - val_loss: 3663.8347 - val_root_mean_squared_error: 60.5290 - learning_rate: 1.5625e-06\n",
            "Epoch 30/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - loss: 3291.8616 - root_mean_squared_error: 56.1767 - val_loss: 3657.2971 - val_root_mean_squared_error: 60.4750 - learning_rate: 1.5625e-06\n",
            "✅ Fold 1 RMSE: 60.4750\n",
            "\n",
            " Training Fold 2/5...\n",
            "Epoch 1/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - loss: 1199175.8750 - root_mean_squared_error: 1063.3163 - val_loss: 4276.7778 - val_root_mean_squared_error: 65.3958 - learning_rate: 1.0000e-04\n",
            "Epoch 2/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 3316.0830 - root_mean_squared_error: 56.5063 - val_loss: 4171.7485 - val_root_mean_squared_error: 64.5881 - learning_rate: 1.0000e-04\n",
            "Epoch 3/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 2435.0515 - root_mean_squared_error: 48.8098 - val_loss: 4090.3059 - val_root_mean_squared_error: 63.9546 - learning_rate: 1.0000e-04\n",
            "Epoch 4/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2601.7417 - root_mean_squared_error: 50.5240 - val_loss: 3781.6675 - val_root_mean_squared_error: 61.4943 - learning_rate: 1.0000e-04\n",
            "Epoch 5/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 3292.5193 - root_mean_squared_error: 56.4194 - val_loss: 4148.3335 - val_root_mean_squared_error: 64.4067 - learning_rate: 1.0000e-04\n",
            "Epoch 6/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - loss: 2939.6223 - root_mean_squared_error: 53.3370 - val_loss: 4160.9341 - val_root_mean_squared_error: 64.5045 - learning_rate: 1.0000e-04\n",
            "Epoch 7/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2978.1445 - root_mean_squared_error: 53.6300 - val_loss: 3691.1318 - val_root_mean_squared_error: 60.7538 - learning_rate: 1.0000e-04\n",
            "Epoch 8/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 2952.1780 - root_mean_squared_error: 53.4329 - val_loss: 3746.4778 - val_root_mean_squared_error: 61.2076 - learning_rate: 1.0000e-04\n",
            "Epoch 9/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - loss: 2990.2834 - root_mean_squared_error: 53.7526 - val_loss: 3532.6565 - val_root_mean_squared_error: 59.4353 - learning_rate: 1.0000e-04\n",
            "Epoch 10/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2729.7959 - root_mean_squared_error: 51.4297 - val_loss: 3646.2781 - val_root_mean_squared_error: 60.3835 - learning_rate: 1.0000e-04\n",
            "Epoch 11/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 2949.3674 - root_mean_squared_error: 53.4144 - val_loss: 3674.4712 - val_root_mean_squared_error: 60.6165 - learning_rate: 1.0000e-04\n",
            "Epoch 12/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2602.4944 - root_mean_squared_error: 50.3621 - val_loss: 3769.2820 - val_root_mean_squared_error: 61.3936 - learning_rate: 1.0000e-04\n",
            "Epoch 13/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 3149.7937 - root_mean_squared_error: 55.1381 - val_loss: 3835.6213 - val_root_mean_squared_error: 61.9315 - learning_rate: 5.0000e-05\n",
            "Epoch 14/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 3379.1042 - root_mean_squared_error: 57.1655 - val_loss: 3731.2253 - val_root_mean_squared_error: 61.0829 - learning_rate: 5.0000e-05\n",
            "Epoch 15/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 2698.4788 - root_mean_squared_error: 51.2395 - val_loss: 4057.9009 - val_root_mean_squared_error: 63.7008 - learning_rate: 5.0000e-05\n",
            "Epoch 16/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - loss: 3364.2717 - root_mean_squared_error: 56.9406 - val_loss: 3659.2166 - val_root_mean_squared_error: 60.4906 - learning_rate: 2.5000e-05\n",
            "Epoch 17/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 3249.3455 - root_mean_squared_error: 55.9948 - val_loss: 3611.5320 - val_root_mean_squared_error: 60.0951 - learning_rate: 2.5000e-05\n",
            "Epoch 18/30\n",
            "\u001b[1m976/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - loss: 3246.9983 - root_mean_squared_error: 55.9727 - val_loss: 3627.1985 - val_root_mean_squared_error: 60.2253 - learning_rate: 2.5000e-05\n",
            "Epoch 19/30\n",
            "\u001b[1m760/976\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3720.0007 - root_mean_squared_error: 59.9097"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-cf2eef073cd1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# pruning_callback = TFKerasPruningCallback(trial, \"val_root_mean_squared_error\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     history = model.fit(train_dataset,\n\u001b[0m\u001b[1;32m     67\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                         verbose=1, validation_data=val_dataset,callbacks=[lr_scheduler])\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 for step, data in zip(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/optional_ops.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    192\u001b[0m       \u001b[0;31m# NOTE: We do not colocate the deserialization of composite tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0;31m# because not all ops are guaranteed to have non-GPU kernels.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mfrom_tensor_list\u001b[0;34m(element_spec, tensor_list)\u001b[0m\n\u001b[1;32m    272\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m   \u001b[0;31m# pylint: disable=g-long-lambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m   return _from_tensor_list_helper(\n\u001b[0m\u001b[1;32m    275\u001b[0m       \u001b[0;32mlambda\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m       tensor_list)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36m_from_tensor_list_helper\u001b[0;34m(decode_fn, element_spec, tensor_list)\u001b[0m\n\u001b[1;32m    225\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcomponent_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_flat_values\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_specs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_spec_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnum_flat_values\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m     \u001b[0mflat_ret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnum_flat_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(spec, value)\u001b[0m\n\u001b[1;32m    273\u001b[0m   \u001b[0;31m# pylint: disable=g-long-lambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m   return _from_tensor_list_helper(\n\u001b[0;32m--> 275\u001b[0;31m       \u001b[0;32mlambda\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m       tensor_list)\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/type_spec.py\u001b[0m in \u001b[0;36m_from_tensor_list\u001b[0;34m(self, tensor_list)\u001b[0m\n\u001b[1;32m    476\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_tensor_specs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m     \"\"\"\n\u001b[0;32m--> 478\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__check_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/type_spec.py\u001b[0m in \u001b[0;36m__check_tensor_list\u001b[0;34m(self, tensor_list)\u001b[0m\n\u001b[1;32m    586\u001b[0m     \u001b[0;34m\"\"\"Raises an exception if tensor_list incompatible w/ flat_tensor_specs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0mexpected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_tensor_specs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m     \u001b[0mspecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspecs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m       raise ValueError(f\"Cannot create a {self.value_type.__name__} from the \"\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/type_spec.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    586\u001b[0m     \u001b[0;34m\"\"\"Raises an exception if tensor_list incompatible w/ flat_tensor_specs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0mexpected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_tensor_specs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m     \u001b[0mspecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspecs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m       raise ValueError(f\"Cannot create a {self.value_type.__name__} from the \"\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/type_spec.py\u001b[0m in \u001b[0;36mtype_spec_from_value\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    987\u001b[0m       \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msupported\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m   \"\"\"\n\u001b[0;32m--> 989\u001b[0;31m   \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_type_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/type_spec.py\u001b[0m in \u001b[0;36m_type_spec_from_value\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m   1009\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcore_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSymbol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[0;31m# Note: we do not include Tensor names when constructing TypeSpecs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrace_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomposite_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompositeTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/core/function/trace_type/trace_type_builder.py\u001b[0m in \u001b[0;36mfrom_value\u001b[0;34m(value, context)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSupportsTracingProtocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     \u001b[0mgenerated_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__tf_tracing_type__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTraceType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m       raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/tensor.py\u001b[0m in \u001b[0;36m__tf_tracing_type__\u001b[0;34m(self, signature_context)\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__tf_tracing_type__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariant\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    745\u001b[0m       \u001b[0mshape_inference_handle_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_data_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_handle_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m       handle_data = (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mdtype\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDType\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;31m# Note: using the intern table directly here as this is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "from tensorflow.keras import mixed_precision\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "#mixed_precision.set_global_policy('mixed_float16')\n",
        "print(tf.test.is_built_with_cuda())  # Should return True\n",
        "print(tf.test.is_gpu_available())\n",
        "tf.config.optimizer.set_jit(False)\n",
        "\n",
        "\n",
        "tf.keras.mixed_precision.set_global_policy('float32')\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "if gpus:\n",
        "    try:\n",
        "        # Set TensorFlow to use only the first GPU\n",
        "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "        print(\"GPU enabled:\", gpus[0])\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "# Define K for K-Fold Cross-Validation\n",
        "K = 5  # Number of folds\n",
        "kf = KFold(n_splits=K, shuffle=True, random_state=42)  # Ensure randomness\n",
        "\n",
        "# Store RMSE for each fold\n",
        "val_rmse_scores = []\n",
        "\n",
        "# Loop through K-folds\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):  # Assuming X_train, y_train are defined\n",
        "    print(f\"\\n Training Fold {fold+1}/{K}...\")\n",
        "\n",
        "    X_train_sub, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "    y_train_sub, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train_sub, y_train_sub))\n",
        "    train_dataset = train_dataset.batch(246).prefetch(tf.data.AUTOTUNE)\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(256).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "\n",
        "    model = keras.Sequential([\n",
        "         keras.layers.Input(shape=(X_train_sub.shape[1],)),\n",
        "        #  keras.layers.Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.005)),\n",
        "        #  keras.layers.Dropout(0.1),\n",
        "          #keras.layers.Dense(256,activation='relu',kernel_regularizer=regularizers.l2(0.005)),\n",
        "        #  keras.layers.Dropout(0.1),\n",
        "          #keras.layers.Dense(128,activation='relu',kernel_regularizer=regularizers.l1_l2(0.001)),\n",
        "          #keras.layers.BatchNormalization(),\n",
        "          #keras.layers.Dropout(0.2),\n",
        "         keras.layers.Dense(64, activation='relu',kernel_regularizer=regularizers.l1_l2(0.0005)),\n",
        "         #keras.layers.BatchNormalization(),\n",
        "         #keras.layers.Dropout(0.1),\n",
        "         keras.layers.Dense(32, activation='relu',kernel_regularizer=regularizers.l1_l2(0.0005)),\n",
        "         #keras.layers.BatchNormalization(),\n",
        "         keras.layers.Dropout(0.2),\n",
        "         keras.layers.Dense(1, activation='linear')\n",
        "                                                      ])  #\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "    model.compile(optimizer=optimizer, loss='mse', metrics=[keras.metrics.RootMeanSquaredError()])\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
        "# pruning_callback = TFKerasPruningCallback(trial, \"val_root_mean_squared_error\")\n",
        "\n",
        "    history = model.fit(train_dataset,\n",
        "                        epochs=30,\n",
        "                        verbose=1, validation_data=val_dataset,callbacks=[lr_scheduler])\n",
        "\n",
        "    # Return validation RMSE as the score to minimize\n",
        "    val_rmse = history.history['val_root_mean_squared_error'][-1]  # Last epoch RMSE\n",
        "    val_rmse_scores.append(val_rmse)  # Store RMSE score\n",
        "    print(f\"✅ Fold {fold+1} RMSE: {val_rmse:.4f}\")\n",
        "\n",
        "\n",
        "mean_rmse = np.mean(val_rmse_scores)\n",
        "std_rmse = np.std(val_rmse_scores)\n",
        "print(f\"Cross-validation RMSE: {mean_rmse:.4f} ± {std_rmse:.4f}\")\n",
        "\n",
        "\n",
        "# final_model = keras.Sequential([\n",
        "#          keras.layers.Input(shape=(X_train.shape[1],)),\n",
        "#         #  keras.layers.Dense(512,activation='relu',kernel_regularizer=regularizers.l2(0.005)),\n",
        "#         #  keras.layers.Dropout(0.1),\n",
        "#           #keras.layers.Dense(256,activation='relu',kernel_regularizer=regularizers.l2(0.005)),\n",
        "#         #  keras.layers.Dropout(0.1),\n",
        "#           keras.layers.Dense(128,activation='relu',kernel_regularizer=regularizers.l1_l2(0.01)),\n",
        "#         #  keras.layers.BatchNormalization(),\n",
        "#           #keras.layers.Dropout(0.1),\n",
        "#          keras.layers.Dense(64, activation='relu',kernel_regularizer=regularizers.l1_l2(0.01)),\n",
        "#          #keras.layers.BatchNormalization(),\n",
        "#          #keras.layers.Dropout(0.1),\n",
        "#          keras.layers.Dense(32, activation='relu',kernel_regularizer=regularizers.l1_l2(0.01)),\n",
        "#          #keras.layers.BatchNormalization(),\n",
        "#          #keras.layers.Dropout(0.01),\n",
        "#          keras.layers.Dense(1, activation='linear')\n",
        "#                                                       ])\n",
        "\n",
        "\n",
        "\n",
        "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "# final_model.compile(optimizer=optimizer, loss='mse', metrics=[keras.metrics.RootMeanSquaredError()])\n",
        "\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# full_train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(256).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# history = final_model.fit(full_train_dataset, epochs=100, verbose=1,callbacks=['early_stopping'])\n",
        "\n",
        "\n",
        "# y_pred=final_model.predict(X_test)\n",
        "\n",
        "# pd.Dataframe(y_pred).to_csv('y_pred.csv',index=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Lk7QbiVJ8iU-"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "VSCode",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
# Define a simple neural network model
def create_model():
    model = Sequential([
        Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
        Dropout(0.2),
        Dense(64, activation='relu'),
        Dropout(0.2),
        Dense(1)  # Regression output (Price prediction)
    ])
    
    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])
    return model

# Train the model
model = create_model()
history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.2, verbose=1)
